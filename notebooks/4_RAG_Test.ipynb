{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a RAG app",
   "id": "cb919a5c92de9859"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Environment Setup\n",
    "\n",
    "This step uses the following libraries:\n",
    "|Library|License|\n",
    "|-|-|\n",
    "| [PyTorch](https://github.com/pytorch/pytorch) | BSD 3-Clause |\n",
    "| [transformers](https://github.com/huggingface/transformers) | Apache 2.0 |\n",
    "| [peft](https://github.com/huggingface/peft) | Apache 2.0 |\n",
    "| [chromadb](https://github.com/chroma-core/chroma) | Apache 2.0 |"
   ],
   "id": "74252cbe630a4fa4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-26T01:21:23.006906Z",
     "start_time": "2025-05-26T01:21:19.901829Z"
    }
   },
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM, PeftConfig\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "import chromadb"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:21:23.092824Z",
     "start_time": "2025-05-26T01:21:23.090430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DOCUMENT    = \"FM5_0\"\n",
    "PDF_PATH    = Path(\"pdfs/raw/fm5-0.pdf\")\n",
    "BASE_MODEL  = Path(\"QuantFactory/Llama-3.2-1B-GGUF\")\n",
    "GGUF_FILE   = \"Llama-3.2-1B.Q8_0.gguf\"\n",
    "CACHE_DIR   = \"hf_cache\"\n",
    "\n",
    "DATA_DIR    = DOCUMENT / BASE_MODEL / \"data\"\n",
    "MODEL_DIR   = DOCUMENT / BASE_MODEL / \"lora\"\n",
    "CHUNKED_DATA = DATA_DIR / \"chunked\" / \"chunked.jsonl\"\n",
    "QA_DATA      = DATA_DIR / \"qa\"       / \"qa_pairs.jsonl\""
   ],
   "id": "4a7177eea128c352",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I'll create a vector database and load the chunks in.",
   "id": "b16917493ee906e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:21:23.249505Z",
     "start_time": "2025-05-26T01:21:23.142045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client     = chromadb.Client()\n",
    "collection = client.create_collection(name=DOCUMENT)"
   ],
   "id": "28df4910e7321231",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:21:23.260707Z",
     "start_time": "2025-05-26T01:21:23.256753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks = []\n",
    "with open(CHUNKED_DATA, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        chunks.append(json.loads(line))"
   ],
   "id": "bae055e15981c4c3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:22:23.257549Z",
     "start_time": "2025-05-26T01:21:23.307885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in chunks:\n",
    "    collection.add(\n",
    "        documents=[chunk[\"text\"]],\n",
    "        ids=[chunk[\"chunk_id\"]]\n",
    "    )"
   ],
   "id": "a7aa0efd9db69c93",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I'll define a helper function to load the chunks from a user's question.",
   "id": "624da7873c9200b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:22:23.329840Z",
     "start_time": "2025-05-26T01:22:23.324303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieve_relevant_chunks(query, n=3):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n\n",
    "    )\n",
    "\n",
    "    contexts = []\n",
    "    for i, doc in enumerate(results[\"documents\"][0]):\n",
    "        context = f\"\\n\\n{doc}\"\n",
    "        contexts.append(context)\n",
    "\n",
    "    return contexts"
   ],
   "id": "9497b949b2c10b2f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And set up the prompt builder used during training.",
   "id": "2b049e0189ff12f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:22:23.385045Z",
     "start_time": "2025-05-26T01:22:23.378699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sys_prompt   = f\" You are an FM-5-0 assistant. Concisely answer the following question.\"\n",
    "sys_role     = \"system\"\n",
    "usr_role     = \"user\"\n",
    "bot_role     = \"assistant\"\n",
    "bos_tok      = \"<|begin_of_text|>\"\n",
    "eot_id_tok   = \"<|eot_id|>\"\n",
    "start_hd_tok = \"<|start_header_id|>\"\n",
    "end_hd_tok   = \"<|end_header_id|>\"\n",
    "eot_tok      = \"<|end_of_text|>\"\n",
    "\n",
    "\n",
    "def build_prompt(sys, context, usr, ans=None):\n",
    "    prompt = f\"{bos_tok}\"\n",
    "    prompt += f\"{start_hd_tok}{sys_role}{end_hd_tok}{context}{sys}{eot_id_tok}\"\n",
    "    prompt += f\"{start_hd_tok}{usr_role}{end_hd_tok}{usr}{eot_id_tok}\"\n",
    "    prompt += f\"{start_hd_tok}{bot_role}{end_hd_tok}\"\n",
    "\n",
    "    if ans is not None:\n",
    "        prompt += f\"{ans}{eot_id_tok}{eot_tok}\"\n",
    "\n",
    "    return prompt"
   ],
   "id": "e9568bb0c0792311",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the model and create a function to automatically retrieve context, build the prompt, and return the result to the user.",
   "id": "3ee05f44ec149adb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:22:23.758192Z",
     "start_time": "2025-05-26T01:22:23.436570Z"
    }
   },
   "cell_type": "code",
   "source": "tok = AutoTokenizer.from_pretrained(MODEL_DIR)",
   "id": "8bee03eba36fc36b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:22:23.780894Z",
     "start_time": "2025-05-26T01:22:23.779065Z"
    }
   },
   "cell_type": "code",
   "source": "final_model_dir = MODEL_DIR / \"final\"",
   "id": "9beab3c254309e9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:22:49.366532Z",
     "start_time": "2025-05-26T01:22:23.840448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_config = PeftConfig.from_pretrained(final_model_dir)\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    final_model_dir,\n",
    "    config=peft_config,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    gguf_file=GGUF_FILE,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.use_cache = False"
   ],
   "id": "39e67bdcf9096b7b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Converting and de-quantizing GGUF tensors...:   0%|          | 0/147 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63d50298c8fe4e97ad586ad07f97b7a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:22:49.393558Z",
     "start_time": "2025-05-26T01:22:49.390940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_answer(query, contexts):\n",
    "    # Combine contexts\n",
    "    combined_context = \"\\n\\n\".join(contexts)\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = build_prompt(sys_role, combined_context, query)\n",
    "\n",
    "    # Generate response\n",
    "    inputs   = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs  = model.generate(**inputs,\n",
    "                               max_new_tokens=256,\n",
    "                               do_sample=True,\n",
    "                               temperature=0.7,\n",
    "                               top_p=0.9,\n",
    "                               repetition_penalty=1.1,\n",
    "                               no_repeat_ngram_size=4,\n",
    "                               eos_token_id=tok.eos_token_id,\n",
    "                               pad_token_id=tok.eos_token_id)\n",
    "    response = tok.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    return response"
   ],
   "id": "59c4fb50f3ffaf25",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:22:49.468988Z",
     "start_time": "2025-05-26T01:22:49.466929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rag_pipeline(query):\n",
    "    contexts = retrieve_relevant_chunks(query)\n",
    "    answer = generate_answer(query, contexts)\n",
    "\n",
    "    return answer"
   ],
   "id": "81f0ead778eb5dfc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test it out.",
   "id": "e7c8470ad57b91b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T01:22:50.322728Z",
     "start_time": "2025-05-26T01:22:49.538645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What are CCIRs?\"\n",
    "r = rag_pipeline(query)\n",
    "print(r)"
   ],
   "id": "e5f1e1dc24d9d047",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specific information identified by a commander that facilitates decision making, directly linking to a current decision. ارزی\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
