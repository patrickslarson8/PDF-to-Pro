{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Parse PDF\n",
    "Step 1 is to parse the PDF. PDFs are kind of just a collection of letters with coordinates, so we have to infer their structure, formatting, and even word groupings."
   ],
   "id": "af20742af3c54a67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Environment Setup\n",
    "Set constants and load secrets from a .env file so we don't store them in the notebook.\n",
    "\n",
    "This step uses the following libraries:\n",
    "|Library|License|\n",
    "|-|-|\n",
    "| [Docling](https://github.com/docling-project/docling]) | MIT |\n",
    "| [EasyOCR](https://github.com/JaidedAI/EasyOCR) | Apache 2.0 |\n",
    "| [python-dotenv](https://github.com/theskumar/python-dotenv) | BSD-3-Clause |\n",
    "| [huggingface_hub](https://github.com/huggingface/huggingface_hub) | Apache 2.0 |\n",
    "| [transformers](https://github.com/huggingface/transformers) | Apache 2.0 |\n",
    "\n"
   ],
   "id": "632112947d7540fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:03:10.885382Z",
     "start_time": "2025-05-22T17:03:07.742914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json, os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    AcceleratorDevice,\n",
    "    AcceleratorOptions,\n",
    "    PdfPipelineOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer"
   ],
   "id": "9222ab1905f22f98",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:03:11.145964Z",
     "start_time": "2025-05-22T17:03:10.957283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DOCUMENT    = \"FM5_0\"\n",
    "PDF_PATH    = Path(\"../pdfs/fm5-0.pdf\")\n",
    "BASE_MODEL  = Path(\"QuantFactory/Llama-3.2-1B-GGUF\")\n",
    "GGUF_FILE   = \"Llama-3.2-1B.Q8_0.gguf\"\n",
    "CACHE_DIR   = \"hf_cache\"\n",
    "\n",
    "load_dotenv()\n",
    "HF_API_KEY = os.environ[\"HF_API_KEY\"]\n",
    "login(HF_API_KEY)\n",
    "\n",
    "MODEL_DIR    = DOCUMENT / BASE_MODEL / \"lora\"\n",
    "DATA_DIR     = DOCUMENT / BASE_MODEL / \"data\"\n",
    "CHUNKED_DATA = DATA_DIR / \"chunked\"  / \"chunked.jsonl\"\n",
    "QA_DATA      = DATA_DIR / \"qa\"       / \"qa_pairs.jsonl\"\n",
    "\n",
    "os.makedirs(CHUNKED_DATA.parent, exist_ok=True)\n",
    "os.makedirs(QA_DATA.parent,      exist_ok=True)\n",
    "os.makedirs(CACHE_DIR,           exist_ok=True)\n",
    "os.makedirs(MODEL_DIR,           exist_ok=True)\n",
    "os.makedirs(DATA_DIR,            exist_ok=True)"
   ],
   "id": "7044d903abdfb849",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tunables\n",
    "Chunk size is the number of tokens in a section of the document we are going to feed into the LLM.\n",
    "\n",
    "The size of the chunks were ultimately dictated by the memory available for training. Bigger might be better in a production system, but it just depends on how the LLM responds to large context windows. Llama 3.2 advertises a context window of 128k but often performance drops off for any context > 10% of the context window. This is an area where more experimentation is needed.\n",
    "\n",
    "Chunk size was set with no overlap so that I could parse the entire document. Ideally we could have some overlap so the LLM can learn how the context chunks fit with each other. I would start at about 25% overlap and tune from there, given I had the resources."
   ],
   "id": "4ed8b1d515eb81c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:03:11.172194Z",
     "start_time": "2025-05-22T17:03:11.169621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks       = []\n",
    "chunk_size   = 512\n",
    "chunk_stride = 512"
   ],
   "id": "8fe0e9492f46cff5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Setup of Docling pipeline. Mostly taken from the [examples in their docs.](https://docling-project.github.io/docling/examples/custom_convert/)\n",
    "\n",
    "This uses EasyOCR on each page to determine word grouping, header, footers, etc. and was generally much better than the alternative. For production, it might be worth the effort to get [RapidOCR](https://github.com/RapidAI/RapidOCR) working, depending on the volume of data needing ingestion."
   ],
   "id": "c7222fb5efa98ba3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:03:11.232614Z",
     "start_time": "2025-05-22T17:03:11.225659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = True\n",
    "pipeline_options.do_table_structure = False\n",
    "pipeline_options.table_structure_options.do_cell_matching = False\n",
    "pipeline_options.ocr_options.lang = [\"en\"]\n",
    "pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "    num_threads=4, device=AcceleratorDevice.AUTO\n",
    ")\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={ InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options) } )"
   ],
   "id": "303e084b413b53e2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once configured, just feed it the PDF and tell it where to save.",
   "id": "ca8a0402e34fa72b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:06:39.647436Z",
     "start_time": "2025-05-22T17:03:11.283675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "converted_pdf = doc_converter.convert(PDF_PATH)\n",
    "pdf_text = converted_pdf.document.export_to_text()"
   ],
   "id": "44ea06c90eab11fb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter `strict_text` has been deprecated and will be ignored.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since we're chunking based on token size, we need to tokenize the text first. This loads the tokenizer.",
   "id": "f7d8ce40d234ea44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:07:02.248513Z",
     "start_time": "2025-05-22T17:06:44.215747Z"
    }
   },
   "cell_type": "code",
   "source": "tok = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir=CACHE_DIR, gguf_file=GGUF_FILE, use_fast=True)",
   "id": "eb6e51e9bbb1e40e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since we're loading a modified model, I'm double-checking that the [Llama3.2 special tokens](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/) are present. They actually aren't in the \"special tokens\" but they are in the vocabulary, which is fine. I'll also check the padding token and set that since some of the tokens seem to be misconfigured.",
   "id": "a14caa19cf18edd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:07:06.727965Z",
     "start_time": "2025-05-22T17:07:06.725942Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Special Tokens: {tok.special_tokens_map}\")",
   "id": "67af477cb70330e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|begin_of_text|>'}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:07:11.275274Z",
     "start_time": "2025-05-22T17:07:11.248223Z"
    }
   },
   "cell_type": "code",
   "source": "v = tok.get_vocab()",
   "id": "7b3d2f1545cf261a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:07:15.811922Z",
     "start_time": "2025-05-22T17:07:15.808968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bos_tok        = \"<|begin_of_text|>\"\n",
    "eot_id_tok     = \"<|eot_id|>\"\n",
    "start_hd_tok   = \"<|start_header_id|>\"\n",
    "eot_tok        = \"<|end_of_text|>\"\n",
    "special_tokens = [bos_tok, eot_id_tok, start_hd_tok, eot_tok]\n",
    "\n",
    "for t in special_tokens:\n",
    "    print(f\"{t} in vocabulary - {t in v}\")"
   ],
   "id": "be476899b32c6f71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|> in vocabulary - True\n",
      "<|eot_id|> in vocabulary - True\n",
      "<|start_header_id|> in vocabulary - True\n",
      "<|end_of_text|> in vocabulary - True\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With the tokenizer set up, I can encode the pdf text and start making the chunks based on the token count.",
   "id": "65273aa9d030e998"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:07:20.663338Z",
     "start_time": "2025-05-22T17:07:20.340784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pdf_encoded = tok(pdf_text)\n",
    "pdf_as_tokens = pdf_encoded.input_ids"
   ],
   "id": "11184bd4610e3d23",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:07:25.337260Z",
     "start_time": "2025-05-22T17:07:25.279143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, start_tok in enumerate(range(0, len(pdf_as_tokens), chunk_stride)):\n",
    "    slice_ids = pdf_as_tokens[start_tok : start_tok + chunk_size]\n",
    "    chunk_text = tok.decode(slice_ids, clean_up_tokenization_spaces=False, skip_special_tokens=True)\n",
    "    chunks.append({\n",
    "        \"chunk_id\": f\"{i:06d}\",\n",
    "        \"text\": chunk_text\n",
    "    })\n",
    "\n",
    "    # Break if we've reached the end of the document\n",
    "    if start_tok + chunk_size >= len(pdf_as_tokens):\n",
    "        break"
   ],
   "id": "bae62b3ef1160a78",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:07:29.884582Z",
     "start_time": "2025-05-22T17:07:29.882291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"There are {len(chunks)} chunks\")\n",
    "print(f\"The first chunk looks like:\\n {chunks[0]}\")"
   ],
   "id": "d0481ad09fd095b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 462 chunks\n",
      "The first chunk looks like:\n",
      " {'chunk_id': '000000', 'text': \"## FM 5-0 PLANNING AND ORDERS PRODUCTION\\n\\nNOVEMBER 2024\\n\\nDISTRIBUTION RESTRICTION:\\n\\nApproved for public release; distribution is unlimited.\\n\\nThis publication supersedes FM 5-0, dated 16 May 2022. HEADQUARTERS, DEPARTMENT OF THE ARMY\\n\\nThis publication is available at the Army Publishing Directorate site (https://armypubs.army.mil) and the Central Army Registry Site (https://atiam.train.army.mil/catalog/dashboard).\\n\\n## PLANNING AND ORDERS PRODUCTION\\n\\n## Contents\\n\\nDISTRIBUTION RESTRICTION: Approved for public release; distribution is unlimited.\\n\\nINTEGRATING PROCESSES SUPPORT TO PLANNING .......................................................  345\\n\\nGlossary ............................................................................................................................................  363\\n\\nReferences ........................................................................................................................................  375\\n\\nIndex ..................................................................................................................................................  381\\n\\n## Figures\\n\\nThis page intentionally left blank.\\n\\n## Preface\\n\\nFM 5-0, Planning and Orders Production , is the Army's comprehensive reference manual for planning. It provides an overview of the fundamentals of planning and details the various planning methodologies for planning operations. It is the Army's doctrinal source for problem solving, the military decision-making process, troop leading procedures, assessment planning, and formats for Army plans and orders.\\n\\nTo comprehend the doctrine contained in this publication, readers must first understand the Army operations described in ADP 3-0 and FM 3-0. Readers must also fully understand the fundaments of command and control and the principles of mission command described in ADP 6-0 and the fundamentals of the operations process found in ADP 5-0.\\n\\nThe principal audience for FM 5-0 includes Army commanders, leaders, and unit staffs (including officers, noncommissioned officers, and Soldiers). Commanders and staffs of Army headquarters serving as a joint task  force  or  multinational  headquarters  should  also  refer  to  applicable  joint  or  multinational  doctrine concerning planning. For joint planning readers should refer to JP 5-0. When planning operations as part of a North Atlantic Treaty Organization operation, readers should refer to Allied Joint Publication 5.\\n\\nCommanders, staffs, and subordinates ensure that their decisions and actions comply with applicable United States, international, and, in\"}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Looks good so we save the chunks and the tokenizer for the next steps.",
   "id": "4ead2a4b43048ddf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:07:34.563910Z",
     "start_time": "2025-05-22T17:07:34.553415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(CHUNKED_DATA, \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in chunks:\n",
    "        f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")"
   ],
   "id": "28cb0a26eca38ffe",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T17:07:40.942576Z",
     "start_time": "2025-05-22T17:07:39.199123Z"
    }
   },
   "cell_type": "code",
   "source": "tok.save_pretrained(MODEL_DIR)",
   "id": "17ba4de9a1e10057",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FM5_0/QuantFactory/Llama-3.2-1B-GGUF/lora/tokenizer_config.json',\n",
       " 'FM5_0/QuantFactory/Llama-3.2-1B-GGUF/lora/special_tokens_map.json',\n",
       " 'FM5_0/QuantFactory/Llama-3.2-1B-GGUF/lora/tokenizer.model',\n",
       " 'FM5_0/QuantFactory/Llama-3.2-1B-GGUF/lora/added_tokens.json',\n",
       " 'FM5_0/QuantFactory/Llama-3.2-1B-GGUF/lora/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
