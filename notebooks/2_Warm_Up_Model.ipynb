{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#todo: explain pipeline steps at a high level here\n",
    "#todo: create requirements.txt and env setup instructions"
   ],
   "id": "7032357c66604528"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:31.393900Z",
     "start_time": "2025-05-15T22:36:29.319727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import accelerate  # Not explicitly used but importing it before transformers prevents some issues w/ pytorch\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback )"
   ],
   "id": "c5fccc773399a888",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load secrets so we don't store them in the notebook.",
   "id": "9bdd13be097b7ab6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:31.465392Z",
     "start_time": "2025-05-15T22:36:31.462657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "HF_API_KEY = os.environ[\"HF_API_KEY\"]\n",
    "\n",
    "DOCUMENT    = \"FM5_0\"\n",
    "PDF_PATH    = Path(\"pdfs/raw/fm5-0.pdf\")\n",
    "BASE_MODEL  = Path(\"QuantFactory/Llama-3.2-1B-GGUF\")\n",
    "GGUF_FILE   = \"Llama-3.2-1B.Q8_0.gguf\"\n",
    "CACHE_DIR   = \"hf_cache\"\n",
    "DATA_DIR    = DOCUMENT / BASE_MODEL / \"data\"\n",
    "MODEL_DIR   = DOCUMENT / BASE_MODEL / \"lora\"\n",
    "CHUNKED_DATA = DATA_DIR / \"chunked\" / \"chunked.jsonl\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ],
   "id": "122e340e591b97a6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:31.810608Z",
     "start_time": "2025-05-15T22:36:31.510091Z"
    }
   },
   "cell_type": "code",
   "source": "ds = load_dataset(\"json\", data_files=CHUNKED_DATA.as_posix(), split=\"train\")",
   "id": "cfa150df6f8e9579",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:31.820700Z",
     "start_time": "2025-05-15T22:36:31.817630Z"
    }
   },
   "cell_type": "code",
   "source": "ds = ds.map(lambda ex: {\"text\": ex[\"text\"]}, remove_columns=ds.column_names)",
   "id": "c3188b677cb47a84",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:32.173779Z",
     "start_time": "2025-05-15T22:36:31.863175Z"
    }
   },
   "cell_type": "code",
   "source": "tok = AutoTokenizer.from_pretrained(MODEL_DIR)",
   "id": "a02a2a12166de344",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:32.195527Z",
     "start_time": "2025-05-15T22:36:32.193659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_group(examples):\n",
    "    tokens = tok(examples[\"text\"])\n",
    "    return tokens"
   ],
   "id": "13595be5df6f73f0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:32.335770Z",
     "start_time": "2025-05-15T22:36:32.249504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized = ds.map(\n",
    "    tokenize_and_group,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"])"
   ],
   "id": "c77cd8532b1d9a4e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:32.356357Z",
     "start_time": "2025-05-15T22:36:32.352704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splits = tokenized.train_test_split(test_size=0.1, seed=42)\n",
    "ds_train = splits[\"train\"]\n",
    "ds_eval  = splits[\"test\"]"
   ],
   "id": "b036df707eaf0d07",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:32.413475Z",
     "start_time": "2025-05-15T22:36:32.411232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tok,\n",
    "    mlm       = False)"
   ],
   "id": "bd3727f133c2e948",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:56.683340Z",
     "start_time": "2025-05-15T22:36:32.468212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    cache_dir   = CACHE_DIR,\n",
    "    gguf_file   = GGUF_FILE,\n",
    "    device_map  = \"auto\",\n",
    "    torch_dtype = torch.float16)\n",
    "model.gradient_checkpointing_enable()"
   ],
   "id": "98a07e832ec66333",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Converting and de-quantizing GGUF tensors...:   0%|          | 0/147 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ce052819e0a4a40be80c7014cb3f9f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:56.703650Z",
     "start_time": "2025-05-15T22:36:56.701695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    r              = 8,\n",
    "    lora_alpha     = 16,\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout   = 0.05,\n",
    "    bias           = \"none\",\n",
    "    task_type      = \"CAUSAL_LM\")"
   ],
   "id": "9fa56eb347465d51",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:57.086325Z",
     "start_time": "2025-05-15T22:36:56.768768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = get_peft_model(model, lora_cfg)\n",
    "model.resize_token_embeddings(len(tok))\n",
    "model.print_trainable_parameters()   # sanity check"
   ],
   "id": "5b25010fe6ffafd4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128261, 2048)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:57.122910Z",
     "start_time": "2025-05-15T22:36:57.107314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "equivalent_batch_size = 128\n",
    "batch_size = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  = MODEL_DIR.as_posix(),\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    gradient_accumulation_steps = int(equivalent_batch_size / batch_size),\n",
    "    learning_rate               = 1e-4,\n",
    "    num_train_epochs            = 1000,\n",
    "    bf16                        = True,\n",
    "    logging_steps               = 2,\n",
    "    save_total_limit            = 5,\n",
    "    dataloader_num_workers      = 8,\n",
    "    dataloader_prefetch_factor  = 2,\n",
    "    label_names                 = [\"labels\"],\n",
    "    metric_for_best_model       = \"eval_loss\",\n",
    "    save_strategy               = \"epoch\",\n",
    "    eval_strategy               = \"epoch\")"
   ],
   "id": "dc312d1bb971b9e1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T22:36:57.178124Z",
     "start_time": "2025-05-15T22:36:57.171138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience  = 5,\n",
    "    early_stopping_threshold = 0.001  )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model         = model,\n",
    "    args          = training_args,\n",
    "    train_dataset = ds_train,\n",
    "    eval_dataset  = ds_eval,\n",
    "    data_collator = data_collator,\n",
    "    callbacks     = [early_stopping])"
   ],
   "id": "bac297dff1fea81f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:20:47.878Z",
     "start_time": "2025-05-15T22:36:57.241934Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "2e456d639a4dcb7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='364' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 364/3000 2:43:20 < 19:49:22, 0.04 it/s, Epoch 91/1000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.297700</td>\n",
       "      <td>3.303851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.183500</td>\n",
       "      <td>3.237642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.118600</td>\n",
       "      <td>3.126435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.003900</td>\n",
       "      <td>2.992384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.911600</td>\n",
       "      <td>2.896091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.785300</td>\n",
       "      <td>2.840994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.761600</td>\n",
       "      <td>2.801643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.733700</td>\n",
       "      <td>2.765341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.712700</td>\n",
       "      <td>2.730158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.688800</td>\n",
       "      <td>2.698385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.596200</td>\n",
       "      <td>2.669285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.612100</td>\n",
       "      <td>2.642688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.567200</td>\n",
       "      <td>2.617306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.545100</td>\n",
       "      <td>2.593145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.524800</td>\n",
       "      <td>2.570984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.486900</td>\n",
       "      <td>2.549699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.477000</td>\n",
       "      <td>2.529067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.467100</td>\n",
       "      <td>2.509898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.467800</td>\n",
       "      <td>2.491488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.432100</td>\n",
       "      <td>2.473558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.406300</td>\n",
       "      <td>2.455472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.399200</td>\n",
       "      <td>2.438982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.330300</td>\n",
       "      <td>2.422180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.362200</td>\n",
       "      <td>2.406401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.345500</td>\n",
       "      <td>2.392697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.358600</td>\n",
       "      <td>2.380004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.270900</td>\n",
       "      <td>2.367832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.304600</td>\n",
       "      <td>2.356756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.272600</td>\n",
       "      <td>2.345462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.317900</td>\n",
       "      <td>2.335583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.254000</td>\n",
       "      <td>2.326333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.248700</td>\n",
       "      <td>2.317893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.215100</td>\n",
       "      <td>2.308725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.221700</td>\n",
       "      <td>2.301058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.197300</td>\n",
       "      <td>2.293481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.214100</td>\n",
       "      <td>2.284853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.233100</td>\n",
       "      <td>2.276844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.182100</td>\n",
       "      <td>2.269810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.165500</td>\n",
       "      <td>2.262920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.177900</td>\n",
       "      <td>2.256363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.144700</td>\n",
       "      <td>2.249407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.161000</td>\n",
       "      <td>2.243572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.180700</td>\n",
       "      <td>2.237026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.114200</td>\n",
       "      <td>2.231994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.047300</td>\n",
       "      <td>2.225121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.121000</td>\n",
       "      <td>2.219423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.087900</td>\n",
       "      <td>2.214928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.113500</td>\n",
       "      <td>2.210277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.044700</td>\n",
       "      <td>2.204624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.101800</td>\n",
       "      <td>2.200140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.049400</td>\n",
       "      <td>2.195948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.085200</td>\n",
       "      <td>2.191469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.008400</td>\n",
       "      <td>2.189646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.026600</td>\n",
       "      <td>2.183917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.083500</td>\n",
       "      <td>2.179631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.019200</td>\n",
       "      <td>2.178375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.000500</td>\n",
       "      <td>2.175606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.017400</td>\n",
       "      <td>2.170407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.102900</td>\n",
       "      <td>2.167472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.990900</td>\n",
       "      <td>2.171640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.941800</td>\n",
       "      <td>2.165873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.008900</td>\n",
       "      <td>2.161033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.034900</td>\n",
       "      <td>2.158352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.898600</td>\n",
       "      <td>2.161668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.032000</td>\n",
       "      <td>2.154540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.973400</td>\n",
       "      <td>2.151953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.967000</td>\n",
       "      <td>2.155276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.950200</td>\n",
       "      <td>2.151984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.888600</td>\n",
       "      <td>2.147315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.015100</td>\n",
       "      <td>2.145673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.982600</td>\n",
       "      <td>2.149463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.940700</td>\n",
       "      <td>2.143628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.891100</td>\n",
       "      <td>2.141659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.963600</td>\n",
       "      <td>2.139372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.889200</td>\n",
       "      <td>2.137760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.904200</td>\n",
       "      <td>2.139401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.923800</td>\n",
       "      <td>2.139699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.868800</td>\n",
       "      <td>2.133981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.962100</td>\n",
       "      <td>2.130363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.917900</td>\n",
       "      <td>2.134398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1.812700</td>\n",
       "      <td>2.139709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.902200</td>\n",
       "      <td>2.128510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.843000</td>\n",
       "      <td>2.127843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.935700</td>\n",
       "      <td>2.134651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.865900</td>\n",
       "      <td>2.131954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.927700</td>\n",
       "      <td>2.124720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.860800</td>\n",
       "      <td>2.128215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.872600</td>\n",
       "      <td>2.130745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.814800</td>\n",
       "      <td>2.129027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.909000</td>\n",
       "      <td>2.125124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.845200</td>\n",
       "      <td>2.125085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=364, training_loss=2.2082887829005062, metrics={'train_runtime': 9830.5233, 'train_samples_per_second': 44.148, 'train_steps_per_second': 0.305, 'total_flos': 1.1849488438419456e+17, 'train_loss': 2.2082887829005062, 'epoch': 91.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:20:48.092Z",
     "start_time": "2025-05-16T01:20:47.901735Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.save_model(MODEL_DIR.as_posix())",
   "id": "42f18fa168adc873",
   "outputs": [],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
