{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Finetune the Model",
   "id": "fea5dcfcb5b2fc84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Environment Setup\n",
    "\n",
    "This step uses the following libraries:\n",
    "|Library|License|\n",
    "|-|-|\n",
    "| [PyTorch](https://github.com/pytorch/pytorch) | BSD 3-Clause |\n",
    "| [python-dotenv](https://github.com/theskumar/python-dotenv) | BSD 3-Clause |\n",
    "| [transformers](https://github.com/huggingface/transformers) | Apache 2.0 |\n",
    "| [datasets](https://github.com/huggingface/datasets) | Apache 2.0 |\n",
    "| [trl](https://github.com/huggingface/trl) | Apache 2.0 |\n",
    "| [peft](https://github.com/huggingface/peft) | Apache 2.0 |\n",
    "| [evaluate](https://github.com/huggingface/evaluate) | Apache 2.0 |\n",
    "| [bert_score](https://github.com/Tiiiger/bert_score) | MIT |\n",
    "| [numpy](https://numpy.org/about/) | Modified BSD |"
   ],
   "id": "f103cc9ef94abcc2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:03.709621Z",
     "start_time": "2025-05-26T00:24:00.856844Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    EarlyStoppingCallback)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:03.722376Z",
     "start_time": "2025-05-26T00:24:03.719528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DOCUMENT    = \"FM5_0\"\n",
    "PDF_PATH    = Path(\"pdfs/raw/fm5-0.pdf\")\n",
    "BASE_MODEL  = Path(\"QuantFactory/Llama-3.2-1B-GGUF\")\n",
    "GGUF_FILE   = \"Llama-3.2-1B.Q8_0.gguf\"\n",
    "CACHE_DIR   = \"hf_cache\"\n",
    "DATA_DIR    = DOCUMENT / BASE_MODEL / \"data\"\n",
    "MODEL_DIR   = DOCUMENT / BASE_MODEL / \"lora\"\n",
    "CHUNKED_DATA = DATA_DIR / \"chunked\" / \"chunked.jsonl\"\n",
    "QA_DATA      = DATA_DIR / \"qa\"       / \"qa_pairs.jsonl\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ],
   "id": "10d8d56c9a612a4e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the dataset and get the tokenizers ready.",
   "id": "329fd97f3b2f2a5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:04.153524Z",
     "start_time": "2025-05-26T00:24:03.813728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "tok.add_bos_token = False\n",
    "tok.add_eos_token = False\n",
    "tok.pad_token = tok.eos_token"
   ],
   "id": "134d5b8ca697ea59",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Configure the model.",
   "id": "78c0a743ade6b78b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:04.175580Z",
     "start_time": "2025-05-26T00:24:04.173821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TEST_PORTION = 0.1\n",
    "IGNORE_ID    = -100\n",
    "MAX_LEN      = 1024"
   ],
   "id": "34e4e9e3348a2461",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And set up the prompt with prompt builders.",
   "id": "69e96d371276f716"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:04.253856Z",
     "start_time": "2025-05-26T00:24:04.252124Z"
    }
   },
   "cell_type": "code",
   "source": "sys_prompt = f\" You are an FM-5-0 assistant. Concisely answer the following question.\"",
   "id": "da0a0a467b6c851b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:04.329154Z",
     "start_time": "2025-05-26T00:24:04.327347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sys_role = \"system\"\n",
    "usr_role = \"user\"\n",
    "bot_role = \"assistant\""
   ],
   "id": "24d3b16c03e599c9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These are already in the tokenizer but being able to reference them will come in handy.",
   "id": "5d549a1b7d00bd2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:04.390923Z",
     "start_time": "2025-05-26T00:24:04.388618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bos_tok      = \"<|begin_of_text|>\"\n",
    "eot_id_tok   = \"<|eot_id|>\"\n",
    "start_hd_tok = \"<|start_header_id|>\"\n",
    "end_hd_tok   = \"<|end_header_id|>\"\n",
    "eot_tok      = \"<|end_of_text|>\""
   ],
   "id": "c6c917c375c565dd",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define some functions to process the data so we can train on it.",
   "id": "dcccbd03a0511c27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:04.470692Z",
     "start_time": "2025-05-26T00:24:04.468396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_prompt(sys, context, usr, ans=None):\n",
    "    prompt  = f\"{bos_tok}\"\n",
    "    prompt += f\"{start_hd_tok}{sys_role}{end_hd_tok}{context}{sys}{eot_id_tok}\"\n",
    "    prompt += f\"{start_hd_tok}{usr_role}{end_hd_tok}{usr}{eot_id_tok}\"\n",
    "    prompt += f\"{start_hd_tok}{bot_role}{end_hd_tok}\"\n",
    "\n",
    "    if ans is not None:\n",
    "        prompt += f\"{ans}{eot_id_tok}{eot_tok}\"\n",
    "\n",
    "    return prompt"
   ],
   "id": "80c26f524210411c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:04.544229Z",
     "start_time": "2025-05-26T00:24:04.542388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def row_to_prompt(row):\n",
    "    return {\"text\": build_prompt(sys_prompt, row['passage'], row['question'], ans=row['answer'])}"
   ],
   "id": "23a4a0c2cba9230a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "INow process the data. I'll use one sample to see how it's handled through the collator and evaluations.",
   "id": "34114a5a1f8a0bc6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:04.961366Z",
     "start_time": "2025-05-26T00:24:04.622073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_ds = load_dataset(\"json\", data_files=QA_DATA.as_posix(), split=\"train\")\n",
    "print(raw_ds[100])"
   ],
   "id": "27a063ac0592f5b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'passage': \"team members identify that a certain population group has a history of not participating in the election process. While knowing that a group does not participate is useful, the planning team understands and explains why the group does not participate. As the planning team maps out the various problems and related causes, it sees that some of the issues are symptoms of a bigger issue. In addition, the team discerns that some problems are outside the scope of their mission. Mapping helps isolate the root cause of problems that the operational approach must address. Figure 4-4 on page 68 is an example of relationship mapping that focuses on the military problems that could be used to further describe a problem frame.\\n\\n## Figure 4-4. Refined operational frame based on strategic frame\\n\\n4-61. The goal of problem framing is to identify obstacles impeding progress toward achieving the desired end state. Effective commanders and planning teams recognize that few problems are solved in isolation, but most are set in relation to other problems within an OE. Rarely is there a single problem facing a command. For example, a unit tasked to neutralize insurgents, enable the host-nation government to expand its influence, and create a capable security force within an assigned area may be faced with the following interrelated problems:\\n\\n- · Lack of sufficient military capabilities to deter armed conflict.\\n- · Host-nation security force systems (including training, logistics, personnel, and pay) are insufficient.\\n- · Host-nation military leaders lack capacity to plan or execute missions.\\n- · Effective insurgent resistance.\\n- · Effective external information campaign.\\n- · Lack of accurate intelligence.\\n- · Civilian casualties.\\n- · The population lacks trust in the host-nation or partner military forces.\\n- · Lack of commitment of regional allies or partner military forces.\\n- · Corruption at the national, district, or provincial level.\\n- · Security along main and alternate supply routes, support area, or intermediate staging bases outside the area of responsibility (AOR).\\n- · The size of the assigned area.\\n- · Tasks assigned versus troops available.\\n- · Limited unity of effort among some unified action partners.\\n\\n## Develop a Problem Frame\\n\\n4-62. The planning team captures its work in a problem frame that describes the set of interrelated problems or  system  of  problems  in  a  narrative  supported  by  visual  models.  The  problem  frame  supports  the commander's dialogue with\", 'question': 'What is the goal of problem framing in military operations?', 'answer': 'To identify obstacles that impede progress toward achieving the desired end state.'}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:04.986512Z",
     "start_time": "2025-05-26T00:24:04.982335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_ds = raw_ds.map(row_to_prompt,\n",
    "                       remove_columns=raw_ds.column_names\n",
    "                       )\n",
    "print(prompt_ds[100])"
   ],
   "id": "90dfc7e0e5ce25e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>team members identify that a certain population group has a history of not participating in the election process. While knowing that a group does not participate is useful, the planning team understands and explains why the group does not participate. As the planning team maps out the various problems and related causes, it sees that some of the issues are symptoms of a bigger issue. In addition, the team discerns that some problems are outside the scope of their mission. Mapping helps isolate the root cause of problems that the operational approach must address. Figure 4-4 on page 68 is an example of relationship mapping that focuses on the military problems that could be used to further describe a problem frame.\\n\\n## Figure 4-4. Refined operational frame based on strategic frame\\n\\n4-61. The goal of problem framing is to identify obstacles impeding progress toward achieving the desired end state. Effective commanders and planning teams recognize that few problems are solved in isolation, but most are set in relation to other problems within an OE. Rarely is there a single problem facing a command. For example, a unit tasked to neutralize insurgents, enable the host-nation government to expand its influence, and create a capable security force within an assigned area may be faced with the following interrelated problems:\\n\\n- · Lack of sufficient military capabilities to deter armed conflict.\\n- · Host-nation security force systems (including training, logistics, personnel, and pay) are insufficient.\\n- · Host-nation military leaders lack capacity to plan or execute missions.\\n- · Effective insurgent resistance.\\n- · Effective external information campaign.\\n- · Lack of accurate intelligence.\\n- · Civilian casualties.\\n- · The population lacks trust in the host-nation or partner military forces.\\n- · Lack of commitment of regional allies or partner military forces.\\n- · Corruption at the national, district, or provincial level.\\n- · Security along main and alternate supply routes, support area, or intermediate staging bases outside the area of responsibility (AOR).\\n- · The size of the assigned area.\\n- · Tasks assigned versus troops available.\\n- · Limited unity of effort among some unified action partners.\\n\\n## Develop a Problem Frame\\n\\n4-62. The planning team captures its work in a problem frame that describes the set of interrelated problems or  system  of  problems  in  a  narrative  supported  by  visual  models.  The  problem  frame  supports  the commander's dialogue with You are an FM-5-0 assistant. Concisely answer the following question.<|eot_id|><|start_header_id|>user<|end_header_id|>What is the goal of problem framing in military operations?<|eot_id|><|start_header_id|>assistant<|end_header_id|>To identify obstacles that impede progress toward achieving the desired end state.<|eot_id|><|end_of_text|>\"}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:05.342702Z",
     "start_time": "2025-05-26T00:24:05.044946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tok_ds    = prompt_ds.map(tok, batched=True, input_columns=[\"text\"], remove_columns=prompt_ds.column_names)\n",
    "print(tok_ds[100])"
   ],
   "id": "48e280605add56b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/462 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "140b730209b949068babc11df957ad62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 128006, 9125, 128007, 9376, 3697, 10765, 430, 264, 3738, 7187, 1912, 706, 264, 3925, 315, 539, 24435, 304, 279, 6355, 1920, 13, 6104, 14392, 430, 264, 1912, 1587, 539, 16136, 374, 5505, 11, 279, 9293, 2128, 31869, 323, 15100, 3249, 279, 1912, 1587, 539, 16136, 13, 1666, 279, 9293, 2128, 14370, 704, 279, 5370, 5435, 323, 5552, 11384, 11, 433, 16008, 430, 1063, 315, 279, 4819, 527, 13803, 315, 264, 11493, 4360, 13, 763, 5369, 11, 279, 2128, 42645, 82, 430, 1063, 5435, 527, 4994, 279, 7036, 315, 872, 9131, 13, 39546, 8779, 43223, 279, 3789, 5353, 315, 5435, 430, 279, 25605, 5603, 2011, 2686, 13, 19575, 220, 19, 12, 19, 389, 2199, 220, 2614, 374, 459, 3187, 315, 5133, 13021, 430, 24400, 389, 279, 6411, 5435, 430, 1436, 387, 1511, 311, 4726, 7664, 264, 3575, 4124, 13, 198, 198, 567, 19575, 220, 19, 12, 19, 13, 432, 4094, 25605, 4124, 3196, 389, 19092, 4124, 198, 198, 19, 12, 5547, 13, 578, 5915, 315, 3575, 59049, 374, 311, 10765, 32116, 3242, 16490, 5208, 9017, 32145, 279, 12974, 842, 1614, 13, 48023, 61427, 323, 9293, 7411, 15641, 430, 2478, 5435, 527, 29056, 304, 31398, 11, 719, 1455, 527, 743, 304, 12976, 311, 1023, 5435, 2949, 459, 56630, 13, 36059, 398, 374, 1070, 264, 3254, 3575, 13176, 264, 3290, 13, 1789, 3187, 11, 264, 5089, 51920, 311, 21277, 553, 88438, 11, 7431, 279, 3552, 12, 99895, 3109, 311, 9407, 1202, 10383, 11, 323, 1893, 264, 13171, 4868, 5457, 2949, 459, 12893, 3158, 1253, 387, 17011, 449, 279, 2768, 958, 9920, 5435, 25, 198, 198, 12, 9787, 68502, 315, 14343, 6411, 17357, 311, 4130, 17903, 12324, 13, 198, 12, 9787, 16492, 12, 99895, 4868, 5457, 6067, 320, 16564, 4967, 11, 43257, 11, 17274, 11, 323, 2343, 8, 527, 39413, 13, 198, 12, 9787, 16492, 12, 99895, 6411, 6164, 6996, 8824, 311, 3197, 477, 9203, 25664, 13, 198, 12, 9787, 48023, 49214, 306, 13957, 13, 198, 12, 9787, 48023, 9434, 2038, 4901, 13, 198, 12, 9787, 68502, 315, 13687, 11478, 13, 198, 12, 9787, 16803, 1122, 48988, 13, 198, 12, 9787, 578, 7187, 37856, 7095, 304, 279, 3552, 12, 99895, 477, 8427, 6411, 8603, 13, 198, 12, 9787, 68502, 315, 15507, 315, 15481, 20724, 477, 8427, 6411, 8603, 13, 198, 12, 9787, 90894, 520, 279, 5426, 11, 9474, 11, 477, 36031, 2237, 13, 198, 12, 9787, 8398, 3235, 1925, 323, 25631, 8312, 11543, 11, 1862, 3158, 11, 477, 29539, 48862, 23963, 4994, 279, 3158, 315, 12014, 320, 32, 878, 570, 198, 12, 9787, 578, 1404, 315, 279, 12893, 3158, 13, 198, 12, 9787, 47571, 12893, 19579, 17312, 2561, 13, 198, 12, 9787, 19439, 31426, 315, 5149, 4315, 1063, 43790, 1957, 8717, 13, 198, 198, 567, 8000, 264, 22854, 16722, 198, 198, 19, 12, 5538, 13, 578, 9293, 2128, 41255, 1202, 990, 304, 264, 3575, 4124, 430, 16964, 279, 743, 315, 958, 9920, 5435, 477, 220, 1887, 220, 315, 220, 5435, 220, 304, 220, 264, 220, 19775, 220, 7396, 220, 555, 220, 9302, 220, 4211, 13, 220, 578, 220, 3575, 220, 4124, 220, 11815, 220, 279, 29094, 596, 21976, 449, 1472, 527, 459, 24342, 12, 20, 12, 15, 18328, 13, 63798, 285, 989, 4320, 279, 2768, 3488, 13, 128009, 128006, 882, 128007, 3923, 374, 279, 5915, 315, 3575, 59049, 304, 6411, 7677, 30, 128009, 128006, 78191, 128007, 1271, 10765, 32116, 430, 3242, 15686, 5208, 9017, 32145, 279, 12974, 842, 1614, 13, 128009, 128001], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:05.371808Z",
     "start_time": "2025-05-26T00:24:05.368681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tok_sample = tok_ds[100]\n",
    "print(\"IDS IN   :\", tok_sample[\"input_ids\"][:40])\n",
    "print(\"MASK     :\", tok_sample[\"attention_mask\"][:40])\n",
    "print(\"TOKENS IN:\", tok.convert_ids_to_tokens(tok_sample[\"input_ids\"][:40]))\n",
    "print(\"TOKENS IN:\", tok.decode(tok_sample[\"input_ids\"][:40], clean_up_tokenization_spaces=True))"
   ],
   "id": "484ebd44986702d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDS IN   : [128000, 128006, 9125, 128007, 9376, 3697, 10765, 430, 264, 3738, 7187, 1912, 706, 264, 3925, 315, 539, 24435, 304, 279, 6355, 1920, 13, 6104, 14392, 430, 264, 1912, 1587, 539, 16136, 374, 5505, 11, 279, 9293, 2128, 31869, 323, 15100]\n",
      "MASK     : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "TOKENS IN: ['<|begin_of_text|>', '<|start_header_id|>', 'system', '<|end_header_id|>', 'team', 'Ġmembers', 'Ġidentify', 'Ġthat', 'Ġa', 'Ġcertain', 'Ġpopulation', 'Ġgroup', 'Ġhas', 'Ġa', 'Ġhistory', 'Ġof', 'Ġnot', 'Ġparticipating', 'Ġin', 'Ġthe', 'Ġelection', 'Ġprocess', '.', 'ĠWhile', 'Ġknowing', 'Ġthat', 'Ġa', 'Ġgroup', 'Ġdoes', 'Ġnot', 'Ġparticipate', 'Ġis', 'Ġuseful', ',', 'Ġthe', 'Ġplanning', 'Ġteam', 'Ġunderstands', 'Ġand', 'Ġexplains']\n",
      "TOKENS IN: <|begin_of_text|><|start_header_id|>system<|end_header_id|>team members identify that a certain population group has a history of not participating in the election process. While knowing that a group does not participate is useful, the planning team understands and explains\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So far so good, we have the desired prompt being tokenized and it de-tokenizes properly. Now I'll check the collator. I'm looking for the entire prompt to be ignored up to the actual assistant response.",
   "id": "a87c780aa36c756"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:05.437831Z",
     "start_time": "2025-05-26T00:24:05.435410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    tokenizer            = tok,\n",
    "    instruction_template = f\"{start_hd_tok}{usr_role}{end_hd_tok}\",\n",
    "    response_template    = f\"{start_hd_tok}{bot_role}{end_hd_tok}\",\n",
    ")"
   ],
   "id": "26eadf5391bcfb6e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pat/PycharmProjects/DunedainAssessment/.venv/lib/python3.12/site-packages/trl/trainer/utils.py:123: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:05.507075Z",
     "start_time": "2025-05-26T00:24:05.503426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = collator([tok_sample])\n",
    "for k,v in batch.items():\n",
    "    print(k, v.shape, v[0][:40])"
   ],
   "id": "e57d27eb89f8769e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 569]) tensor([128000, 128006,   9125, 128007,   9376,   3697,  10765,    430,    264,\n",
      "          3738,   7187,   1912,    706,    264,   3925,    315,    539,  24435,\n",
      "           304,    279,   6355,   1920,     13,   6104,  14392,    430,    264,\n",
      "          1912,   1587,    539,  16136,    374,   5505,     11,    279,   9293,\n",
      "          2128,  31869,    323,  15100])\n",
      "attention_mask torch.Size([1, 569]) tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "labels torch.Size([1, 569]) tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First part looks good, all of the context that's injected is ignored.",
   "id": "688bac0f4d3c01a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:05.577408Z",
     "start_time": "2025-05-26T00:24:05.573843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for k,v in batch.items():\n",
    "    print(k, v.shape, v[0][-40:])"
   ],
   "id": "98f5ae98bf61a5a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 569]) tensor([  4320,    279,   2768,   3488,     13, 128009, 128006,    882, 128007,\n",
      "          3923,    374,    279,   5915,    315,   3575,  59049,    304,   6411,\n",
      "          7677,     30, 128009, 128006,  78191, 128007,   1271,  10765,  32116,\n",
      "           430,   3242,  15686,   5208,   9017,  32145,    279,  12974,    842,\n",
      "          1614,     13, 128009, 128001])\n",
      "attention_mask torch.Size([1, 569]) tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "labels torch.Size([1, 569]) tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   1271,  10765,  32116,\n",
      "           430,   3242,  15686,   5208,   9017,  32145,    279,  12974,    842,\n",
      "          1614,     13, 128009, 128001])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And looks like the very end of this sample contains the actual tokens (non -100 values). I'll detokenize those to make sure the entire answer is included.",
   "id": "957784c237180b95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:05.646227Z",
     "start_time": "2025-05-26T00:24:05.643092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "labels = batch[\"labels\"][0].tolist()\n",
    "last_mask_index = len(labels) - 1 - labels[::-1].index(IGNORE_ID)\n",
    "masked_label = tok.decode(labels[last_mask_index + 1:], skip_special_tokens=True)\n",
    "print(f\"The collator only left this unmasked: {masked_label}\")\n",
    "print(f\"Is only the answer unmasked?: {masked_label == raw_ds[100][\"answer\"]}\")"
   ],
   "id": "10c6fccd1fc66606",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collator only left this unmasked: To identify obstacles that impede progress toward achieving the desired end state.\n",
      "Is only the answer unmasked?: True\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With data processing nailed down, I can split the data into a training and testing dataset and prepare for training. To create a more robust training cycle that leverages all data, I would utilize 10-fold cross-validation with 2 folds set to testing data while tuning the hyperparameters. After I'm happy with the hyperparameters, I'll train using all the data. For this though, I'm just going to use some typical good values for the hyperparameters.",
   "id": "a8c3d3522a72c663"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T00:24:05.719496Z",
     "start_time": "2025-05-26T00:24:05.714360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splits     = tok_ds.train_test_split(TEST_PORTION, seed=42)\n",
    "tok_train  = splits[\"train\"]\n",
    "tok_test   = splits[\"test\"]"
   ],
   "id": "dd2c930fb43b7a9d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we load the model and the LoRA adapter.\n",
    "\n",
    "Ideally this would be dead simple with SFTTrainer, but it doesn't support custom metrics yet (https://github.com/huggingface/trl/issues/862) so we have to do everything manually. I'm using gradient checkpointing just because I ran out of memory while training on my personal GPU."
   ],
   "id": "92666caf9905ad1e"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-26T00:24:05.779553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            cache_dir=CACHE_DIR,\n",
    "            gguf_file=GGUF_FILE,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16)\n",
    "base_model.gradient_checkpointing_enable()"
   ],
   "id": "1f7946ddf8deeb4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lora_cfg = LoraConfig(\n",
    "    r              = 8,\n",
    "    lora_alpha     = 16,\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout   = 0.05,\n",
    "    bias           = \"none\",\n",
    "    task_type      = \"CAUSAL_LM\")\n",
    "lora_model = get_peft_model(base_model, lora_cfg)\n",
    "lora_model.print_trainable_parameters()  # sanity check"
   ],
   "id": "7646caf1ac6cf7fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Before training, I'll set up some metrics for evaluation.\n",
    "\n",
    " - F1: This is span-wise F1 (from SQUAD) shows how well the prediction and truth match if we treat them as a \"bag of tokens\".\n",
    " - Perplexity: I like to look at this over loss because you can interpret it as how \"confident\" the model is for the next token. E.g. a perplexity of ~2 means the model is considering bet\n",
    " - BERT Score: This is a good one to help understand how close the meaning of the output is to the label. Since it compares the BERT embeddings of the prediction and label, the embeddings of similar words are more closely aligned than disparate words.\n",
    "\n",
    "There are some others I would like to use to gain as much insight as possible, but I omitted for simplicity here."
   ],
   "id": "5fb597a4e9e1a0f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bert_metric   = evaluate.load(\"bertscore\", cache_dir=CACHE_DIR)\n",
    "squad_metric  = evaluate.load(\"squad\", cache_dir=CACHE_DIR)"
   ],
   "id": "53d37749346972df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_preds) -> dict:\n",
    "    preds  = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    losses = eval_preds.losses\n",
    "\n",
    "    cleaned_labels = np.where(labels != IGNORE_ID, labels, tok.pad_token_id)\n",
    "    cleaned_preds  = np.where(preds  != IGNORE_ID, preds,  tok.pad_token_id)\n",
    "\n",
    "    decoded_preds  = tok.batch_decode(cleaned_preds.tolist(), skip_special_tokens=True)\n",
    "    decoded_labels = tok.batch_decode(cleaned_labels.tolist(), skip_special_tokens=True)\n",
    "\n",
    "    squad_preds = [\n",
    "        {\"id\": str(i), \"prediction_text\": p}\n",
    "        for i, p in enumerate(decoded_preds)\n",
    "    ]\n",
    "    squad_refs = [\n",
    "        {\n",
    "            \"id\": str(i),\n",
    "            \"answers\": {\"text\": [decoded_labels[i]], \"answer_start\": [0]}\n",
    "        }\n",
    "        for i in range(len(decoded_labels))\n",
    "    ]\n",
    "    squad_results = squad_metric.compute(\n",
    "        predictions = squad_preds,\n",
    "        references  = squad_refs\n",
    "    )\n",
    "\n",
    "    bert_results = bert_metric.compute(\n",
    "        predictions = decoded_preds,\n",
    "        references  = decoded_labels,\n",
    "        lang        = \"en\",\n",
    "        model_type  = \"distilbert-base-uncased\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"perplexity\":      np.mean(np.exp(losses)),\n",
    "        \"bert_precision\":  np.mean(bert_results[\"precision\"]),\n",
    "        \"bert_recall\":     np.mean(bert_results[\"recall\"]),\n",
    "        \"bert_f1\":         np.mean(bert_results[\"f1\"]),\n",
    "        \"qa_f1\":           squad_results[\"f1\"],\n",
    "    }"
   ],
   "id": "4872268a6e5fdd6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "All that's left is to set up the training loop and train the model.",
   "id": "7364474cc18806ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lora_model.config.use_cache = False\n",
    "lora_model.generation_config.pad_token_id = tok.pad_token_id"
   ],
   "id": "ef8f0bb71a3c9c6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir                  = MODEL_DIR,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 32,\n",
    "    num_train_epochs            = 10,\n",
    "    learning_rate               = 2e-4,\n",
    "    logging_steps               = 1,\n",
    "    save_steps                  = 1,\n",
    "    save_total_limit            = 10,\n",
    "    neftune_noise_alpha         = 0.1,\n",
    "    bf16                        = True,\n",
    "    bf16_full_eval              = True,\n",
    "    save_strategy               = \"epoch\",\n",
    "    eval_strategy               = \"epoch\",\n",
    "    report_to                   = \"none\",\n",
    "    label_names                 = [\"labels\"],\n",
    "    metric_for_best_model       = \"eval_loss\",\n",
    "    load_best_model_at_end      = True,\n",
    "    eval_on_start               = True,\n",
    "    eval_accumulation_steps     = 10,\n",
    "    include_for_metrics         = [\"loss\"],\n",
    "    predict_with_generate       = True,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience  = 1,\n",
    "    early_stopping_threshold = 0.001,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model           = lora_model,\n",
    "    args            = args,\n",
    "    train_dataset   = tok_train,\n",
    "    eval_dataset    = tok_test,\n",
    "    data_collator   = collator,\n",
    "    callbacks       = [early_stopping],\n",
    "    compute_metrics = compute_metrics,\n",
    ")"
   ],
   "id": "ba1335d4a0043888",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "1396f499eeef5dca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now I'll compare the base model to the lora model. Since loading the lora adapters modifies the model in-place (even though I used a new variable), we need to load the base model again.",
   "id": "2e26578766a4c879"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_model_2 = AutoModelForCausalLM.from_pretrained(\n",
    "            BASE_MODEL,\n",
    "            cache_dir=CACHE_DIR,\n",
    "            gguf_file=GGUF_FILE,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16)"
   ],
   "id": "6000f64e95a5a1b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "qa_data = []\n",
    "with open(QA_DATA, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        qa_data.append(json.loads(line))"
   ],
   "id": "8ec6469fcab926bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A quick test to see if the model internalized any of the data or just learned how to copy from the context. I'll do this by setting the context to nothing and asking a question that I know was in the training data.",
   "id": "24888b878bee2ac0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "question = qa_data[7][\"question\"]\n",
    "answer   = qa_data[7][\"answer\"]\n",
    "print(f\"{question}\\n{answer}\")"
   ],
   "id": "913a6f5834533d4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "context = \"\"\n",
    "prompt =  build_prompt(sys_prompt, context, question)\n",
    "\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(lora_model.device)\n",
    "out = base_model_2.generate(**inputs,\n",
    "                           max_new_tokens=256,\n",
    "                           do_sample=True,\n",
    "                           temperature=0.7,\n",
    "                           top_p=0.9,\n",
    "                           repetition_penalty=1.1,\n",
    "                           no_repeat_ngram_size=4,\n",
    "                           eos_token_id=tok.eos_token_id,\n",
    "                           pad_token_id=tok.eos_token_id)\n",
    "print(tok.decode(out[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True))"
   ],
   "id": "831cb049f4fdeec7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "context = \"\"\n",
    "prompt =  build_prompt(sys_prompt, context, question)\n",
    "\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(lora_model.device)\n",
    "out = lora_model.generate(**inputs,\n",
    "                           max_new_tokens=256,\n",
    "                           do_sample=True,\n",
    "                           temperature=0.7,\n",
    "                           top_p=0.9,\n",
    "                           repetition_penalty=1.1,\n",
    "                           no_repeat_ngram_size=4,\n",
    "                           eos_token_id=tok.eos_token_id,\n",
    "                           pad_token_id = tok.eos_token_id)\n",
    "print(tok.decode(out[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True))"
   ],
   "id": "47add3b7c5680270",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_out = base_model_2.generate(**inputs,\n",
    "                               max_new_tokens=256,\n",
    "                               do_sample=True,\n",
    "                               temperature=0.7,\n",
    "                               top_p=0.9,\n",
    "                               repetition_penalty=1.1,\n",
    "                               no_repeat_ngram_size=4,\n",
    "                               eos_token_id=tok.eos_token_id,\n",
    "                               pad_token_id = tok.eos_token_id)\n",
    "print(tok.decode(base_out[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True))"
   ],
   "id": "7c225b0e59af9916",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "context  = qa_data[7][\"passage\"]\n",
    "prompt   =  build_prompt(sys_prompt, context, question)\n",
    "inputs   = tok(prompt, return_tensors=\"pt\").to(lora_model.device)\n",
    "lora_out = lora_model.generate(**inputs,\n",
    "                               max_new_tokens=256,\n",
    "                               do_sample=True,\n",
    "                               temperature=0.7,\n",
    "                               top_p=0.9,\n",
    "                               repetition_penalty=1.1,\n",
    "                               no_repeat_ngram_size=4,\n",
    "                               eos_token_id=tok.eos_token_id,\n",
    "                               pad_token_id=tok.eos_token_id)\n",
    "print(tok.decode(lora_out[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True))"
   ],
   "id": "f2c150779960e2be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_model_path = MODEL_DIR / \"final\"\n",
    "lora_model.save_pretrained(final_model_path.as_posix())"
   ],
   "id": "a1f912139eca0ddb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
